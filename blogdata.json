[
  {
    "id": 1,
    "body": "<p>Data Insight is the valuable and useful information, and conclusions drawn from the processing and analysis of raw data. This information can present for example customer buying habits, acquisition cost, sales and operation of companies&hellip;. In general, data insight is the process of turning raw data into useful, big and surprising information, which then can help to transform company practices into fresh, cost-effective solutions leading to more efficient operations. In this article, I will present the 3 steps process that I normally use to turn data into useful insight for teams or companies:</p><h2>Step 1: Understanding data points (WHAT?)</h2><p>In the first step, we should gain basic knowledge of the data that we have, which columns are useful to answer our questions, and which data is available to resolve our issues. In the big dataset, we should only focus on the key drivers to answer your questions. For example, if you&rsquo;re looking to answer why your profit is decreasing, you should look for data like expenses and revenue.</p><p>Also, we can calculate and visualize data to better understand data points:</p><h2><strong>Step 2: Meaning of data aligned with the context (WHY?)</strong></h2><p>From step 1, we have to apply context to data points to gain the real meaning of data aligned with the reality of your business. It sounds ambiguous but the below example will give you better ideas of what does it mean. I still remember one of my mistakes not applying the context to the analyzed data when I first started as Data Analyst. The revenue chart showed that this quarter&apos;s revenue was 15 million compared to only around 10&ndash;12 million in previous quarters. I immediately sent an email to my manager saying that the performance of the sales team was exceptional. Turned out that the marketing team spent a few million dollars to run promotional campaigns for that quarter to push the revenue of the company. Therefore, missing the context of the company, I inaccurately gave a conclusion to the data. The context is important to understand what&rsquo;s going on with the data you analyze.</p><h2>Step 3: Recommendation (ACTION)</h2><p>Based on an understanding of the meaning of the data and the context of your business, what are your recommendations and actions to improve the efficiency of your business? You have to get into the final step to have the full data insight process.</p><blockquote>    <p>Therefore, Data insight = WHAT + WHY + ACTION</p></blockquote><h2>Alright, let&rsquo;s apply 3 steps above to the example:</h2><p><em>Step 1: What (data points)</em></p><p>For example, I visualized the data in the chart below, showing the changes in customers buying in stores across North America. You can easily see that the number of existing/retained customers has been reduced while there is increasing in new customers.</p><p><em>Step 2: Why (context)</em></p><p>If we apply the context, we would understand better why this is the case. Actually, last few quarters, the marketing team only created campaigns to attract new customers to the store in order to create a higher customer base. The strategy is not wrong, but the existing/retained customers may feel left out and choose to go to other stores with more attractive promotions designed for them.</p><p><em>Step 3: Action</em></p><p>From what we understand about the data and context, we can conclude some actions and next steps to resolve the current issue. Obviously, some campaigns to target loyal and existing customers will be ideal here. We can suggest to the marketing team to create a loyalty program. Sometimes we are not sure about solutions, we should have a discussion with business partners. Do not try to make recommendations if you are not so sure because your solutions may be unrealistic and impossible to implement, which will make your whole effort of data insight become worthless.</p>",
    "title": "Strategies to turn data into insights",
    "category": "strategy",
    "thumnail": "https://miro.medium.com/v2/resize:fit:640/format:webp/0*V7utLb_eqexHR4uQ.jpg"
  },
  {
    "id": 2,
    "body": "<p>Why is the data model important?</p><p>Despite the rise of the modern data stack, data modelling is still an important topic. There is an increasingly wide range of data sources and data end users nowadays. End users have different expectations of what they want to see out of data. To bridge the gap between data sources and end users, data modelling plays a vital role to organize the data in such a way that different end users can extract and use data to answer their business questions.</p><p><br></p><p>Four most common approaches:</p><ol>    <li>Normalized Modelling:</li>    <li>Denormalized Modelling (Ralph Kimball): designed for business functions, redundant</li>    <li>Data Vault 2.0 (Hubs, Links &amp; Satellites): Hubs &amp; Links: Metadata, Satellites hold descriptive values and context</li>    <li>One big Table (Flatten Straight to Wide Tables)</li></ol><p>Things to Consider:</p>",
    "title": "Different Data Modelling in Data Warehouse",
    "category": "technology",
    "thumnail": "https://cdn-images-1.medium.com/max/720/0*RKm48pF6kpwDOVg3.jpg"
  },
  {
    "id": 3,
    "body": "<p name=\"516d\">In the previous article &ldquo;Strategies to turn data into insights&rdquo;, I have discussed the 3 steps. A question may be raised that what kind of charts or maps I should visualize to identify data points in step 1 of the data insight process.</p><p name=\"91c3\">In this article, I will present 9 common techniques to help you better identify data points in your dataset:</p><p name=\"3c81\">Technique 1: Change Over Time (Time Series): Normally we can find trends in time series data, whether the trend is up, down, or seasonal over time.</p><figure name=\"522a\"><img src=\"https://cdn-images-1.medium.com/max/720/0*UskHTHoaBemjPRod.png\"></figure><figure name=\"b379\"><img src=\"https://cdn-images-1.medium.com/max/720/0*tL_lC2L4T4XGFdea.jpg\"></figure><p name=\"b0ee\">Technique 2: Identify the relationship between variables, and find the correlation between them. Some of the useful charts for this technique are heatmaps, live charts or scatter plots.</p><figure name=\"1c1a\"><img src=\"https://cdn-images-1.medium.com/max/720/1*Uzd4y7VqmaNHs2vVoBeKgQ.png\">    <figcaption>Scatter plot captured from website <a href=\"https://www.mathsisfun.com/data/scatter-xy-plots.html\" rel=\"nofollow noopener\" target=\"_blank\">https://www.mathsisfun.com/data/scatter-xy-plots.html</a> showing the relationship between sales of ice cream and temperature</figcaption></figure><figure name=\"bd64\"><img src=\"https://cdn-images-1.medium.com/max/720/0*mWugTT6C4GyDizBJ.png\">    <figcaption>Heatmap captured from <a href=\"https://www.balbix.com/insights/cyber-risk-heat-map/\" rel=\"nofollow noopener\" target=\"_blank\">https://www.balbix.com/insights/cyber-risk-heat-map/</a> showing Risk impact heat map to show the likelihood of a risk event happening vs. business impact of such that event</figcaption></figure><p name=\"4448\">Technique 3: in some situations, there are variables meeting with each other at one point, creating an intersection. As a result, we may draw some stories when there is a change. The most popular example is the break-even chart below.</p><figure name=\"06c8\"><img src=\"https://cdn-images-1.medium.com/max/720/0*Z9YBT2dewLuJxOOj.jpg\"></figure><p name=\"8301\">Technique 4: Forecasting</p><p name=\"de59\">The most common type of this chart is to project the sales revenue of the company in the next 6 months or 1 year based on historical data and trends. By forecasting the revenue, the company can align better the resources and expenses.&nbsp;</p><figure name=\"9e3a\"><img src=\"https://cdn-images-1.medium.com/max/720/0*q34s2r-H65B5dKqI.png\"></figure><p name=\"2914\">Technique 5: Compare and contrast</p><p name=\"35bb\">In this technique, we can compare and find the patterns and behaviours in different variables/groups. For example, we can segment and compare the 2 groups of customers: loyal and lost customers. By doing it, we can focus on patterns that can drive loyal customers and stop/prevent issues that can cause customers to leave the company.</p><p name=\"be73\">Technique 6: Drill Down Technique</p><p name=\"c737\">Drill down technique allow us to have different angles/views of an issue: from high level to the very detail. For example, instead of analyzing the product, we can understand how the category/subcategory is doing.</p><figure name=\"ecad\"><img src=\"https://cdn-images-1.medium.com/max/720/1*_MbUCxOXzOiKnFGjbY7y-w.png\"></figure><p name=\"195a\">Technique 7: Zoom in/Zoom out</p><p name=\"5e65\">This is usually a feature that allows you to zoom in and zoom out of a chart. The advantage of this is similar to drill down, which allows to view data at various levels of granularity, especially when the data under observation is huge and can be organized into layers for easier interpretation.&nbsp;</p><p name=\"10a1\">Technique 8: Cluster</p><p name=\"2266\"><br></p><p name=\"fdbd\">Technique 9: Outlier</p>",
    "title": "9 Techniques to find stories ofÂ data",
    "category": "technology",
    "thumnail": "https://cdn-images-1.medium.com/max/720/0*mWugTT6C4GyDizBJ.png"
  },
  {
    "id": 4,
    "body": "<p name=\"70e8\">Learning programming skills from a young age can make a big difference.</p><h3 name=\"ed94\">At what age can a child start&nbsp;coding?</h3><p name=\"391c\">Just like when learning a new language, it&rsquo;s both better and easier to learn coding skills from a young age. &ldquo;There is research into children being able to learn aspects of coding from the age of three, as well as during kindergarten and early elementary school,&rdquo; the Raspberry Pi Foundation rep explains. &ldquo;Younger children typically learn coding by programming graphical symbols. Then they move on to block-based coding and text-based coding.&rdquo; Start by introducing your child to a simple game, and then work up to more complex games as they develop their skills.</p><h3 name=\"04ee\"><strong>1. Code&nbsp;Karts</strong></h3><figure name=\"726b\"><img src=\"https://cdn-images-1.medium.com/max/720/0*gVS6aQxnA3RyrzUf\"></figure><ul>    <li name=\"7d5f\"><strong>Platform:&nbsp;</strong>iOS, Android, Kindle Fire</li>    <li name=\"1ae9\"><strong>Pricing:&nbsp;</strong>Free</li>    <li name=\"3c08\"><strong>Age range:</strong> 4 and up</li></ul><p name=\"272a\">Think your preschooler is too young to learn to code? Think again. <a href=\"https://www.amazon.com/Code-Karts-Pre-coding-in-Preschool/dp/B071FJBZZG\" rel=\"nofollow noskim noopener\" target=\"_blank\">Code Karts</a> makes coding for kids as young as four years old possible, using racetrack-themed logical puzzles and over 70 different levels (as well as two different game modes).</p><h3 name=\"e06c\">2. ScratchJr/Scratch</h3><figure name=\"55f9\"><img src=\"https://cdn-images-1.medium.com/max/720/0*E6kX-11pN4Yb5y_S\"></figure><ul>    <li name=\"df85\"><strong>Platform:&nbsp;</strong>iOS, Android, Kindle Fire</li>    <li name=\"f10c\"><strong>Pricing:&nbsp;</strong>Free</li>    <li name=\"35ad\"><strong>Age range:&nbsp;</strong>5 to 7</li></ul><p name=\"a8d7\">Perfect for young learners, <a href=\"https://www.scratchjr.org/\" rel=\"nofollow noopener\" target=\"_blank\">ScratchJr</a> is introductory programming language that lets kids create their own interactive stories and games. By programming blocks and animating their characters, users are exposed to new math and language concepts, as well as the building blocks of programming.</p><h3 name=\"5194\">3. CodeSpark Academy</h3><figure name=\"6d5e\"><img src=\"https://cdn-images-1.medium.com/max/720/0*R-pWHUaLMiL_CS-i\"></figure><ul>    <li name=\"06a9\"><strong>Platform:&nbsp;</strong>Browser, iOS, Android, Kindle Fire</li>    <li name=\"7b43\"><strong>Pricing:</strong> Free to start, with <a href=\"https://accounts.codespark.com/gift?_ga=2.206556757.732638830.1648688627-986881025.1648688627\" rel=\"nofollow noopener\" target=\"_blank\">plans starting at $5.56/month</a></li>    <li name=\"cb8f\"><strong>Age range:&nbsp;</strong>5 to 9</li></ul><p name=\"cbc8\">Developed by research from MIT, Princeton, and Carnegie Mellon, <a href=\"https://codespark.com/\" rel=\"nofollow noopener\" target=\"_blank\">codeSpark Academy</a> teaches all of the fundamentals of computer programming through daily activities, puzzles, and games personalized for your child&rsquo;s skill level. Best of all, new content is released monthly, so your kids will never get bored.</p><h3 name=\"8ce0\">4. Tynker</h3><figure name=\"8bdb\"><img src=\"https://cdn-images-1.medium.com/max/720/0*Zzew1ozMUdb3Eg1R\"></figure><ul>    <li name=\"0da0\"><strong>Platform:</strong> Browser, iOS, Android</li>    <li name=\"22ca\"><strong>Pricing:&nbsp;</strong>Free to start, with <a href=\"https://www.tynker.com/parents/pricing\" rel=\"nofollow noopener\" target=\"_blank\">plans starting at $8.50/month</a></li>    <li name=\"6613\"><strong>Age range:&nbsp;</strong>5 to 18</li></ul><p name=\"de73\">Designed to make programming accessible and fun for kids as young as five years old, <a href=\"https://www.tynker.com/\" rel=\"nofollow noopener\" target=\"_blank\">Tynker</a> gives kids the ability to write interactive stories, program drones, and explore other STEM topics, regardless of their prior experience. It can even be played offline without internet connectivity, so kids can play on the go.</p><h3 name=\"ab68\">5. CodeCombat</h3><figure name=\"1409\"><img src=\"https://cdn-images-1.medium.com/max/720/0*zSqszVSg8wMNbu3N\"></figure><ul>    <li name=\"152c\"><strong>Platform:&nbsp;</strong>Browser</li>    <li name=\"912d\"><strong>Pricing:&nbsp;</strong>Free to start, with <a href=\"https://codecombat.com/premium\" rel=\"nofollow noopener\" target=\"_blank\">plans starting at $9.99/month</a></li>    <li name=\"9078\"><strong>Age range:&nbsp;</strong>9 and up</li></ul><p name=\"4b92\">One of the most straightforward coding games for kids, <a href=\"https://codecombat.com/\" rel=\"nofollow noopener\" target=\"_blank\">CodeCombat</a> combines the world of fantasy (think: knights and dragons) with the basics of coding. Each lesson is introduced as another chapter in the overarching storyline of the CodeCombat universe, which is what makes this game so fun.</p><h3 name=\"53bb\">6. Minecraft</h3><figure name=\"3b44\"><img src=\"https://cdn-images-1.medium.com/max/720/0*JCReSVll-dlBonzl\"></figure><ul>    <li name=\"6e40\"><strong>Platform:&nbsp;</strong>Mac, Nintendo Switch, Nintendo Wii U, PlayStation 3, PlayStation 4, PlayStation Vita, Windows, Xbox 360, Xbox One</li>    <li name=\"bce1\"><strong>Pricing:&nbsp;</strong>$6.99 (<a href=\"https://apps.apple.com/us/app/minecraft/id479516143\" rel=\"nofollow noopener\" target=\"_blank\">app</a>), $26.95 (<a href=\"https://www.minecraft.net/en-us/store/minecraft-java-edition\" rel=\"nofollow noopener\" target=\"_blank\">PC and Mac</a>)</li>    <li name=\"8502\"><strong>Age range:&nbsp;</strong>10 and up</li></ul><p name=\"c0e2\">Perhaps the best known coding game for kids, <a href=\"https://www.minecraft.net/en-us\" rel=\"nofollow noopener\" target=\"_blank\">Minecraft</a> is all about using blocks to build objects, interact with other characters, and progress to new levels. It may sound simple, but it gives players the freedom to create and explore almost limitlessly (while also fulfilling missions and going after goals).</p><h3 name=\"56fd\">7. Projects</h3><figure name=\"9818\"><img src=\"https://cdn-images-1.medium.com/max/720/0*RiQj6SkDwCUIQY5m\"></figure><ul>    <li name=\"921f\"><strong>Platform:&nbsp;</strong>Browser</li>    <li name=\"5ca1\"><strong>Pricing:&nbsp;</strong>Free</li>    <li name=\"451f\"><strong>Age range:&nbsp;</strong>12 and up</li></ul><p name=\"4b00\">Also created by Raspberry Pi Foundation, <a href=\"https://projects.raspberrypi.org/en\" rel=\"nofollow noopener\" target=\"_blank\">Projects</a> is a catalog of more than 250 free coding projects for young people to explore and further enhance their coding skills. From creating a stress ball app, to coding musical instruments, to making a racing game, there are tons of projects available, and each project is designed to be completed in an hour.</p><h3 name=\"362f\">8. Khan&nbsp;Academy</h3><figure name=\"1f35\"><img src=\"https://cdn-images-1.medium.com/max/720/0*Md53JD8Mbpztt2nU.jpg\"></figure><ul>    <li name=\"2843\"><strong>Platform:&nbsp;</strong>Browser</li>    <li name=\"e6eb\"><strong>Pricing:&nbsp;</strong>Free</li>    <li name=\"b604\"><strong>Age range: 3&ndash;7</strong></li></ul>",
    "title": "Different Data Modelling in Data Warehouse",
    "category": "motivation",
    "thumnail": "https://cdn-images-1.medium.com/max/720/0*E6kX-11pN4Yb5y_S"
  },
  {
    "id": 5,
    "body": "<h3>Six Validation Techniques to Improve Your Data Quality</h3><p>Boost the data quality of your data warehouse with six practical techniques.</p><p>Have you ever had a set of reports that were distributed for years only to have your business users discover that the reports have been wrong all along and consequently lose trust in your data warehouse environment? Gaining trust is the foundation of user adoption and business value of your data management program.</p><p>Taking data quality seriously can be difficult if agility and speed-to-market are the name of the game for your business users. This is a lesson that is very costly to learn the hard way. How do you prevent these issues from occurring? After all, it&apos;s not like you didn&apos;t have validation checks as part of your standard process.</p><p>Full data-quality frameworks can be time-consuming and costly to establish. The costs are lower if you institute your data quality steps upfront in your original design process, but it is a valuable exercise to review and overhaul your data quality practices if you only have basic checks in place today.</p><p>Here are a few data validation techniques that may be missing in your environment.</p><p><strong>Source system loop back verification:</strong> In this technique, you perform aggregate-based verifications of your subject areas and ensure it matches the originating data source. For example, if you are pulling information from a billing system, you can take total billing for a single day and ensure totals match on the data warehouse as well. Although this seems like an easy validation to perform, enterprises don&apos;t often use it. The technique can be one of the best ways to ensure completeness of data.</p><p><strong>Ongoing source-to-source verification:</strong> What happens if a data integrity issue from your source system gets carried over to your data warehouse? Ultimately, you can expect that the data warehouse will share in the negative exposure of that issue, even if the data warehouse was not the cause. One way you can help catch these problems before they fester is to have an approximate verification across multiple source systems or compare similar information at different stages of your business life cycle. This can be a meaningful way to catch non data warehouse issues and protect the integrity of the information you send from the data warehouse.</p><p><strong>Data-Issue tracking:</strong> By centrally tracking all of your issues in one place, you can find recurring issues, reveal riskier subject areas, and help ensure proper preventive measures have been applied. Making it easy for business users and IT to input issues and report on them is required for effective tracking.</p><p><strong>Data certification:</strong> Performing up-front data validation before you add it to your data warehouse, including the use of data profiling tools, is a very important technique. It can add noticeable time to integrate new data sources into your data warehouse, but the long-term benefits of this step greatly enhance the value of the data warehouse and trust in your information.</p><p><strong>Statistics collection:</strong> Ensuring you maintain statistics for the full life cycle of your data will arm you with meaningful information to create alarms for unexpected results. Whether you have an in-house-developed statistics collection process or you rely upon metadata captured with your transformation program, you need to ensure you can set alarms based upon trending. For example, if your loads are typically a specific size day in and day out and suddenly the volume shrinks in half, this should set off an alert and a full investigation should occur for such events. This situation may not trigger your typical checks, so it&apos;s a great way to find those difficult-to-catch situations on an automated basis.</p><p><strong>Workflow management:</strong> Thinking properly about data quality while you design your data integration flows and overall workflows can allow for catching issues quickly and efficiently. Performing checks along the way gives you more advanced options to resolve the issue quickly. One example of this is to have strong stop and restart processes built into your workflow so that as an issue is found in the loading process, it can trigger a restart and determine if the issue was environment based. Enabling autocorrection of common challenges related to performance and environmental factors. It also can enable data quality checks that are only possible at the sub-task level during the processing window.</p><p><strong>A Final Word</strong></p><p>Although there may not be any truly foolproof way to prevent all data integrity issues, making data quality part of the DNA of your environment will go a long way in gaining the trust of your user community.</p>",
    "title": "Data Validation and Reconciliation",
    "category": "technology",
    "thumnail": "https://www.dnb.com/content/dam/english/image-library/Modernization/illustrations/img-resources-data-validation.jpg"
  },
  {
    "id": 6,
    "body": "<p style=\"margin:0in;font-family:Calibri;font-size:11.0pt\" lang=\"en-US\">Understanding the company&apos;s entire data warehouse architecture&nbsp; can feel overwhelming. But when you cut out the noise, you will find there&apos;s typically 1 or 2 high level approaches being followed: ETL or ELT</p><p style=\"margin:0in;font-family:Calibri;font-size:11.0pt\" lang=\"en-US\">Understanding the differences between the two will help people undersatnd better your architecture and give you the confidence to hop into the core of both approaches. ELT and ETL are two similar but different strategies for moving data from a source system to a destination and apply business logic</p><p style=\"margin:0in;font-family:Calibri;font-size:11.0pt\" lang=\"en-US\">&nbsp;</p><p style=\"margin:0in;font-family:Calibri;font-size:11.0pt\" lang=\"en-US\">In this video, I will explain both of these things are, the differences and give my take on which one is actually better</p><p style=\"margin:0in;font-family:Calibri;font-size:11.0pt\" lang=\"en-US\">&nbsp;</p><p style=\"margin:0in;font-family:Calibri;font-size:11.0pt\" lang=\"en-US\">First we start with ETL which stand for Extract, Transform, Load. As acronym implies in this scenario we first extract the data from the source system and is followed immediately by applying transformations aka business logic to that extracted data we then finally load that transformed data into some final tables where they can be consumed by end users and reporting tools if you have something like Power BI or Tableau. A Key point in this approach is that the transform process will only run against data that is extracted during the same run and to make this possible this extracted source data is first moved typically to a temporary storage location. You may hear it refer to a staging layer. What all it means is that just another database or schema being used for this purpose. <br> The data in the staging layer is then truncated or cleared out to get ready for next run. Example tools you see when working with ETL processes are Microsoft SSIS, Informatica, etc</p><p style=\"margin:0in;font-family:Calibri;font-size:11.0pt\" lang=\"en-US\">&nbsp;</p><p style=\"margin:0in;font-family:Calibri;font-size:11.0pt\" lang=\"en-US\">Alright, so what&apos;s about ELT? You may already guess the letters represent exact same concept but now it just different orders, the workflow is Extract, Load, Transform. This typically seen a more modern approach for reason discussed here shortly</p><p style=\"margin:0in;font-family:Calibri;font-size:11.0pt\" lang=\"en-US\">&nbsp;</p><p style=\"margin:0in;font-family:Calibri;font-size:11.0pt\" lang=\"en-US\">ELT scenario first we extract the data, but this time instead of landing in the temporary staging area, the data is loaded into the permanent tables or permanent location. This is you may see the concept of data lake, this table is continue to grow with each run, and this is gonna act as essenssially a historical reference for all data for particular source</p><p style=\"margin:0in;font-family:Calibri;font-size:11.0pt\" lang=\"en-US\">&nbsp;</p><p style=\"margin:0in;font-family:Calibri;font-size:11.0pt\" lang=\"en-US\">Finally, transformation is gonna apply to a much larger dataset, which also includes logic to create custom data models. Now because the extracted data is not cleared with each run, you can extract the data as frequently as you like, and run your transformation at a completely separate time. There is nothing tying these activities together like you see in ETL process where all the steps need to happen in the same run. </p><p style=\"margin:0in;font-family:Calibri;font-size:11.0pt\" lang=\"en-US\">&nbsp;</p><p style=\"margin:0in;font-family:Calibri;font-size:11.0pt\" lang=\"en-US\">Also, rather than relying the staging tables for the latest data, you can instead apply filters on the new larger data table within your transformation logic. </p><p style=\"margin:0in;font-family:Calibri;font-size:11.0pt\" lang=\"en-US\">&nbsp;</p><p style=\"margin:0in;font-family:Calibri;font-size:11.0pt\" lang=\"en-US\">For those who choose ELT approach you&apos;ll often see multiple specialized tools within your workflow rather than just having one that handles everything together. So a few of the many examples are fivetran or stitch for the extract and loading, or you can use the streaming tools like apache kafka or AWS firehose to extract data in more realtime fashion and then obviously dbt is my favorite for the transformation component of ELT process.</p><p style=\"margin:0in;font-family:Calibri;font-size:11.0pt\" lang=\"en-US\">&nbsp;</p><p style=\"margin:0in;font-family:Calibri;font-size:11.0pt\" lang=\"en-US\">&nbsp;</p><p style=\"margin:0in;font-family:Calibri;font-size:11.0pt\" lang=\"en-US\">Now before we can settle on this answer there are few things we neeed first to point out or discuss. As mentioned, the main difference between these approaches come with the loading and transformation step. Wth the datastore becoming much cheaper and company continuing move to the cloud, there is essensially no limit how much you can store, the modern db is also incredibly powerful and can parse over insane amounts of data very efficiently. Having said that they are not very well suited for a lot of truncating or reloading or individual record updating. </p><p style=\"margin:0in;font-family:Calibri;font-size:11.0pt\" lang=\"en-US\">&nbsp;</p><p style=\"margin:0in;font-family:Calibri;font-size:11.0pt\" lang=\"en-US\">Considering the design for the db availability and the trend towards more specialized tools and the capability of modern cloud DB, if I start from scratch today, I will go with ELT approach. So this approach will leave you with more tools, also add more complexity to your architecture, but it&apos;s a more scalable model and benefits outweigh those downsides in my opinion. </p><p style=\"margin:0in;font-family:Calibri;font-size:11.0pt\" lang=\"en-US\">&nbsp;</p><p></p><p style=\"margin:0in;font-family:Calibri;font-size:11.0pt\" lang=\"en-US\">ETL is by no mean death and there are still so many companies still using this particular approach. Frankly it can be massive undertaking to try to change this type of infrastructure which means there are still going to be a ton of companies still successfully every day running their architecture in an ETL format.</p>",
    "title": "ETL vs ELT",
    "category": "technology",
    "thumnail": "https://www.holistics.io/blog/content/images/2018/02/etl-vs-elt.png"
  },
  {
    "id": 7,
    "body": "<p>Why is the data model important?</p><p>Despite the rise of the modern data stack, data modelling is still an important topic. There is an increasingly wide range of data sources and data end users nowadays. End users have different expectations of what they want to see out of data. To bridge the gap between data sources and end users, data modelling plays a vital role to organize the data in such a way that different end users can extract and use data to answer their business questions.</p><p><br></p><p>Four most common approaches:</p><ol>    <li>Normalized Modelling:</li>    <li>Denormalized Modelling (Ralph Kimball): designed for business functions, redundant</li>    <li>Data Vault 2.0 (Hubs, Links &amp; Satellites): Hubs &amp; Links: Metadata, Satellites hold descriptive values and context</li>    <li>One big Table (Flatten Straight to Wide Tables)</li></ol><p>Things to Consider:</p>",
    "title": "Write better SQL",
    "category": "technology",
    "thumnail": "https://ploomber.io/images/blog/jupysql-header_hu159b9d2cdcd245da79dc77da8d4a60f1_244087_3122d3f6af74d4297eda48f708f9f71f.webp"
  },
  {
    "id": 8,
    "body": "<p>Why is the data model important?</p><p>Despite the rise of the modern data stack, data modelling is still an important topic. There is an increasingly wide range of data sources and data end users nowadays. End users have different expectations of what they want to see out of data. To bridge the gap between data sources and end users, data modelling plays a vital role to organize the data in such a way that different end users can extract and use data to answer their business questions.</p><p><br></p><p>Four most common approaches:</p><ol>    <li>Normalized Modelling:</li>    <li>Denormalized Modelling (Ralph Kimball): designed for business functions, redundant</li>    <li>Data Vault 2.0 (Hubs, Links &amp; Satellites): Hubs &amp; Links: Metadata, Satellites hold descriptive values and context</li>    <li>One big Table (Flatten Straight to Wide Tables)</li></ol><p>Things to Consider:</p>",
    "title": "System Design",
    "category": "technology",
    "thumnail": "https://cdn-icons-png.flaticon.com/512/8759/8759045.png"
  },
  {
    "id": 9,
    "body": "<p>Why is the data model important?</p><p>Despite the rise of the modern data stack, data modelling is still an important topic. There is an increasingly wide range of data sources and data end users nowadays. End users have different expectations of what they want to see out of data. To bridge the gap between data sources and end users, data modelling plays a vital role to organize the data in such a way that different end users can extract and use data to answer their business questions.</p><p><br></p><p>Four most common approaches:</p><ol>    <li>Normalized Modelling:</li>    <li>Denormalized Modelling (Ralph Kimball): designed for business functions, redundant</li>    <li>Data Vault 2.0 (Hubs, Links &amp; Satellites): Hubs &amp; Links: Metadata, Satellites hold descriptive values and context</li>    <li>One big Table (Flatten Straight to Wide Tables)</li></ol><p>Things to Consider:</p>",
    "title": "How to approach SQL Window Functions",
    "category": "technology",
    "thumnail": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQfmef53gA2cxpmD-kj0q89M6XeUOJnB942jQ&usqp=CAU"
  }
]